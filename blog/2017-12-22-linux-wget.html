<!DOCTYPE html><html lang="en" data-theme="retro"><head><meta charSet="utf-8"/><link rel="preconnect" href="https://cdn.jsdelivr.net"/><link rel="stylesheet" href="/_next/static/css/4640374ce090311f.css" data-precedence="next.js"/><link rel="preload" as="style" href="https://cdn.jsdelivr.net/gh/orioncactus/pretendard/dist/web/variable/pretendardvariable.css"/><link rel="preload" as="script" href="https://www.googletagmanager.com/gtag/js?id=G-XR7V7MF96T"/><title>리눅스 wget 명령어</title><meta name="description" content="&lt;h3&gt;설명&lt;/h3&gt;
&lt;p&gt;CUI환경에서 파일을 다운받을때 사용한다.&lt;/p&gt;
&lt;h3&gt;도움말&lt;/h3&gt;
"/><link rel="author" href="https://github.io/ddalpange"/><meta name="author" content="ddalpange"/><meta name="keywords" content="Linux,Wget"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="any"/><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/orioncactus/pretendard/dist/web/variable/pretendardvariable.css"/><script src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js" nomodule=""></script></head><body class="flex flex-col items-center h-[100svh]"><div class="w-full max-w-3xl flex-grow"><div class="navbar"><div class="flex-1"><a class="p-2 hover:underline md:text-2xl font-bold flex gap-4 items-center" href="/">Yozzing Blog</a></div><div class="flex-none"><div class="tabs"><a class="tab" href="/engineering">Engineering</a><a class="tab" href="/essay">Essay</a><a class="tab" href="https://github.com/ddalpange"><div class="avatar"><div class="w-8 rounded-full"><img alt="yozzing" src="/blog/images/profile.jpeg"/></div></div></a></div></div></div><div class="p-4 pb-10"><article class="prose max-w-full flex flex-col items-center"><span class="mb-2">over 5 years ago</span><h1>리눅스 wget 명령어</h1><div><div class="prose self-center"><h3>설명</h3>
<p>CUI환경에서 파일을 다운받을때 사용한다.</p>
<h3>도움말</h3>
<pre><code>$ wget -help

GNU Wget 1.17.1, a non-interactive network retriever.
Usage: wget [OPTION]... [URL]...

Mandatory arguments to long options are mandatory for short options too.

Startup:
  -V,  --version                   display the version of Wget and exit
  -h,  --help                      print this help
  -b,  --background                go to background after startup
  -e,  --execute=COMMAND           execute a `.wgetrc&#x27;-style command

Logging and input file:
  -o,  --output-file=FILE          log messages to FILE
  -a,  --append-output=FILE        append messages to FILE
  -d,  --debug                     print lots of debugging information
  -q,  --quiet                     quiet (no output)
  -v,  --verbose                   be verbose (this is the default)
  -nv, --no-verbose                turn off verboseness, without being quiet
       --report-speed=TYPE         output bandwidth as TYPE.  TYPE can be bits
  -i,  --input-file=FILE           download URLs found in local or external FILE
  -F,  --force-html                treat input file as HTML
  -B,  --base=URL                  resolves HTML input-file links (-i -F)
                                     relative to URL
       --config=FILE               specify config file to use
       --no-config                 do not read any config file
       --rejected-log=FILE         log reasons for URL rejection to FILE

Download:
  -t,  --tries=NUMBER              set number of retries to NUMBER (0 unlimits)
       --retry-connrefused         retry even if connection is refused
  -O,  --output-document=FILE      write documents to FILE
  -nc, --no-clobber                skip downloads that would download to
                                     existing files (overwriting them)
  -c,  --continue                  resume getting a partially-downloaded file
       --start-pos=OFFSET          start downloading from zero-based position OFFSET
       --progress=TYPE             select progress gauge type
       --show-progress             display the progress bar in any verbosity mode
  -N,  --timestamping              don&#x27;t re-retrieve files unless newer than
                                     local
       --no-if-modified-since      don&#x27;t use conditional if-modified-since get
                                     requests in timestamping mode
       --no-use-server-timestamps  don&#x27;t set the local file&#x27;s timestamp by
                                     the one on the server
  -S,  --server-response           print server response
       --spider                    don&#x27;t download anything
  -T,  --timeout=SECONDS           set all timeout values to SECONDS
       --dns-timeout=SECS          set the DNS lookup timeout to SECS
       --connect-timeout=SECS      set the connect timeout to SECS
       --read-timeout=SECS         set the read timeout to SECS
  -w,  --wait=SECONDS              wait SECONDS between retrievals
       --waitretry=SECONDS         wait 1..SECONDS between retries of a retrieval
       --random-wait               wait from 0.5*WAIT...1.5*WAIT secs between retrievals
       --no-proxy                  explicitly turn off proxy
  -Q,  --quota=NUMBER              set retrieval quota to NUMBER
       --bind-address=ADDRESS      bind to ADDRESS (hostname or IP) on local host
       --limit-rate=RATE           limit download rate to RATE
       --no-dns-cache              disable caching DNS lookups
       --restrict-file-names=OS    restrict chars in file names to ones OS allows
       --ignore-case               ignore case when matching files/directories
  -4,  --inet4-only                connect only to IPv4 addresses
  -6,  --inet6-only                connect only to IPv6 addresses
       --prefer-family=FAMILY      connect first to addresses of specified family,
                                     one of IPv6, IPv4, or none
       --user=USER                 set both ftp and http user to USER
       --password=PASS             set both ftp and http password to PASS
       --ask-password              prompt for passwords
       --no-iri                    turn off IRI support
       --local-encoding=ENC        use ENC as the local encoding for IRIs
       --remote-encoding=ENC       use ENC as the default remote encoding
       --unlink                    remove file before clobber

Directories:
  -nd, --no-directories            don&#x27;t create directories
  -x,  --force-directories         force creation of directories
  -nH, --no-host-directories       don&#x27;t create host directories
       --protocol-directories      use protocol name in directories
  -P,  --directory-prefix=PREFIX   save files to PREFIX/..
       --cut-dirs=NUMBER           ignore NUMBER remote directory components

HTTP options:
       --http-user=USER            set http user to USER
       --http-password=PASS        set http password to PASS
       --no-cache                  disallow server-cached data
       --default-page=NAME         change the default page name (normally
                                     this is &#x27;index.html&#x27;.)
  -E,  --adjust-extension          save HTML/CSS documents with proper extensions
       --ignore-length             ignore &#x27;Content-Length&#x27; header field
       --header=STRING             insert STRING among the headers
       --max-redirect              maximum redirections allowed per page
       --proxy-user=USER           set USER as proxy username
       --proxy-password=PASS       set PASS as proxy password
       --referer=URL               include &#x27;Referer: URL&#x27; header in HTTP request
       --save-headers              save the HTTP headers to file
  -U,  --user-agent=AGENT          identify as AGENT instead of Wget/VERSION
       --no-http-keep-alive        disable HTTP keep-alive (persistent connections)
       --no-cookies                don&#x27;t use cookies
       --load-cookies=FILE         load cookies from FILE before session
       --save-cookies=FILE         save cookies to FILE after session
       --keep-session-cookies      load and save session (non-permanent) cookies
       --post-data=STRING          use the POST method; send STRING as the data
       --post-file=FILE            use the POST method; send contents of FILE
       --method=HTTPMethod         use method &quot;HTTPMethod&quot; in the request
       --body-data=STRING          send STRING as data. --method MUST be set
       --body-file=FILE            send contents of FILE. --method MUST be set
       --content-disposition       honor the Content-Disposition header when
                                     choosing local file names (EXPERIMENTAL)
       --content-on-error          output the received content on server errors
       --auth-no-challenge         send Basic HTTP authentication information
                                     without first waiting for the server&#x27;s
                                     challenge

HTTPS (SSL/TLS) options:
       --secure-protocol=PR        choose secure protocol, one of auto, SSLv2,
                                     SSLv3, TLSv1 and PFS
       --https-only                only follow secure HTTPS links
       --no-check-certificate      don&#x27;t validate the server&#x27;s certificate
       --certificate=FILE          client certificate file
       --certificate-type=TYPE     client certificate type, PEM or DER
       --private-key=FILE          private key file
       --private-key-type=TYPE     private key type, PEM or DER
       --ca-certificate=FILE       file with the bundle of CAs
       --ca-directory=DIR          directory where hash list of CAs is stored
       --crl-file=FILE             file with bundle of CRLs
       --random-file=FILE          file with random data for seeding the SSL PRNG
       --egd-file=FILE             file naming the EGD socket with random data

HSTS options:
       --no-hsts                   disable HSTS
       --hsts-file                 path of HSTS database (will override default)

FTP options:
       --ftp-user=USER             set ftp user to USER
       --ftp-password=PASS         set ftp password to PASS
       --no-remove-listing         don&#x27;t remove &#x27;.listing&#x27; files
       --no-glob                   turn off FTP file name globbing
       --no-passive-ftp            disable the &quot;passive&quot; transfer mode
       --preserve-permissions      preserve remote file permissions
       --retr-symlinks             when recursing, get linked-to files (not dir)

FTPS options:
       --ftps-implicit                 use implicit FTPS (default port is 990)
       --ftps-resume-ssl               resume the SSL/TLS session started in the control connection when
                                         opening a data connection
       --ftps-clear-data-connection    cipher the control channel only; all the data will be in plaintext
       --ftps-fallback-to-ftp          fall back to FTP if FTPS is not supported in the target server
WARC options:
       --warc-file=FILENAME        save request/response data to a .warc.gz file
       --warc-header=STRING        insert STRING into the warcinfo record
       --warc-max-size=NUMBER      set maximum size of WARC files to NUMBER
       --warc-cdx                  write CDX index files
       --warc-dedup=FILENAME       do not store records listed in this CDX file
       --no-warc-compression       do not compress WARC files with GZIP
       --no-warc-digests           do not calculate SHA1 digests
       --no-warc-keep-log          do not store the log file in a WARC record
       --warc-tempdir=DIRECTORY    location for temporary files created by the
                                     WARC writer

Recursive download:
  -r,  --recursive                 specify recursive download
  -l,  --level=NUMBER              maximum recursion depth (inf or 0 for infinite)
       --delete-after              delete files locally after downloading them
  -k,  --convert-links             make links in downloaded HTML or CSS point to
                                     local files
       --convert-file-only         convert the file part of the URLs only (usually known as the basename)
       --backups=N                 before writing file X, rotate up to N backup files
  -K,  --backup-converted          before converting file X, back up as X.orig
  -m,  --mirror                    shortcut for -N -r -l inf --no-remove-listing
  -p,  --page-requisites           get all images, etc. needed to display HTML page
       --strict-comments           turn on strict (SGML) handling of HTML comments

Recursive accept/reject:
  -A,  --accept=LIST               comma-separated list of accepted extensions
  -R,  --reject=LIST               comma-separated list of rejected extensions
       --accept-regex=REGEX        regex matching accepted URLs
       --reject-regex=REGEX        regex matching rejected URLs
       --regex-type=TYPE           regex type (posix|pcre)
  -D,  --domains=LIST              comma-separated list of accepted domains
       --exclude-domains=LIST      comma-separated list of rejected domains
       --follow-ftp                follow FTP links from HTML documents
       --follow-tags=LIST          comma-separated list of followed HTML tags
       --ignore-tags=LIST          comma-separated list of ignored HTML tags
  -H,  --span-hosts                go to foreign hosts when recursive
  -L,  --relative                  follow relative links only
  -I,  --include-directories=LIST  list of allowed directories
       --trust-server-names        use the name specified by the redirection
                                     URL&#x27;s last component
  -X,  --exclude-directories=LIST  list of excluded directories
  -np, --no-parent                 don&#x27;t ascend to the parent directory

Mail bug reports and suggestions to &lt;bug-wget@gnu.org&gt;
</code></pre>
<h3>활용 예</h3>
<p>영상 다운로드서버에 올라가있는 영상들이 정상적인지를 체크하기위해 한번씩 다운로드해준다.</p>
<p><em>list.txt</em></p>
<pre><code> http://111.111.11.1/assets/prod/1629/ORIGINAL/1629_1504176178478.ts
 http://111.111.11.1/assets/prod/1630/ORIGINAL/1630_1504176206826.ts
 http://1111.111.11.1/assets/prod/1672/ORIGINAL/1672_1505466475322.ts
 엄청많아 100개가 넘어 ..
</code></pre>
<pre><code>$ wget -i list.txt -o log --no-verbose
</code></pre>
<p>list에 써져있는 파일들을 차례대로 다운로드한 후 로그를 남긴다(성공여부만 필요)</p>
<p>-i 인풋
-o 아웃풋
--no-verbose 다 남길 필요없다.</p>
<p><em>log</em></p>
<pre><code>2017-12-22 17:17:53 URL:http://111.111.11.1/assets/prod/1629/ORIGINAL/1629_1504176178478.ts [16958540/16958540] -&gt; &quot;1629_1504176178478.ts.2&quot; [1]
2017-12-22 17:17:56 URL:http://111.111.11.1/assets/prod/1630/ORIGINAL/1630_1504176206826.ts [32955836/32955836] -&gt; &quot;1630_1504176206826.ts.2&quot; [1]
2017-12-22 17:17:59 URL:http://111.111.11.1/assets/prod/1672/ORIGINAL/1672_1505466475322.ts [32954520/32954520] -&gt; &quot;1672_1505466475322.ts.2&quot; [1]
2017-12-22 17:18:03 URL:http://111.111.11.1/assets/uplus-assets/prod/1686/ORIGINAL/1686_1505969267788.ts [32950572/32950572] -&gt; &quot;1686_1505969267788.ts.2&quot; [1]
http://111.111.11.1/assets/prod/1692/ORIGINAL/1692_1506023232326203376.ts:
2017-12-22 17:18:03 ERROR 404: Not Found.
FINISHED --2017-12-22 17:18:03--
Total wall clock time: 12s
Downloaded: 4 files, 110M in 11s (9.61 MB/s)
</code></pre>
<p>없으면 저렇게 404 Not Found 뜬다.</p>
<p>20개정도 브라우저에 입력하다가 빡쳐서 찾았다.</p>
</div></div><div class="w-5/6"><div id="disqus_thread"></div></div></article></div></div><script src="/_next/static/chunks/webpack-54cd5478b621d6d8.js" async=""></script><script src="/_next/static/chunks/0b7c6ed9-60e4a7f8e855a262.js" async=""></script><script src="/_next/static/chunks/474-c8c5eb3e5e298e46.js" async=""></script><script src="/_next/static/chunks/main-app-7b248d73399883e9.js" async=""></script></body></html><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"0:\"$L1\"\n"])</script><script>self.__next_f.push([1,"2:I{\"id\":\"9638\",\"name\":\"\",\"chunks\":[\"272:webpack-54cd5478b621d6d8\",\"776:0b7c6ed9-60e4a7f8e855a262\",\"474:474-c8c5eb3e5e298e46\"],\"async\":false}\n4:I{\"id\":\"3644\",\"name\":\"\",\"chunks\":[\"272:webpack-54cd5478b621d6d8\",\"776:0b7c6ed9-60e4a7f8e855a262\",\"474:474-c8c5eb3e5e298e46\"],\"async\":false}\n6:I{\"id\":\"2070\",\"name\":\"\",\"chunks\":[\"131:131-718a907043a28311\",\"185:app/layout-e83a0904fdc2baed\"],\"async\":false}\n7:I{\"id\":\"7131\",\"name\":\"\",\"chunks\":[\"131:131-718a907043a28311\",\"931:app/page-59d2145bcde12638\"],\"async\":false}\n8:I{"])</script><script>self.__next_f.push([1,"\"id\":\"830\",\"name\":\"\",\"chunks\":[\"272:webpack-54cd5478b621d6d8\",\"776:0b7c6ed9-60e4a7f8e855a262\",\"474:474-c8c5eb3e5e298e46\"],\"async\":false}\n9:I{\"id\":\"2079\",\"name\":\"\",\"chunks\":[\"272:webpack-54cd5478b621d6d8\",\"776:0b7c6ed9-60e4a7f8e855a262\",\"474:474-c8c5eb3e5e298e46\"],\"async\":false}\n"])</script><script>self.__next_f.push([1,"1:[\"$\",\"$L2\",null,{\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/blog/2017-12-22-linux-wget\",\"initialTree\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"2017-12-22-linux-wget\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"2017-12-22-linux-wget\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialHead\":\"$L3\",\"globalErrorComponent\":\"$4\",\"notFound\":[\"$@5\",[\"$\",\"html\",null,{\"lang\":\"en\",\"data-theme\":\"retro\",\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://cdn.jsdelivr.net\"}],[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"type\":\"text/css\",\"href\":\"https://cdn.jsdelivr.net/gh/orioncactus/pretendard/dist/web/variable/pretendardvariable.css\"}]]}],[\"$\",\"body\",null,{\"className\":\"flex flex-col items-center h-[100svh]\",\"children\":[[\"$\",\"$L6\",null,{\"src\":\"https://www.googletagmanager.com/gtag/js?id=G-XR7V7MF96T\",\"strategy\":\"afterInteractive\"}],[\"$\",\"$L6\",null,{\"id\":\"google-analytics\",\"strategy\":\"afterInteractive\",\"children\":\"\\n          window.dataLayer = window.dataLayer || [];\\n          function gtag(){window.dataLayer.push(arguments);}\\n          gtag('js', new Date());\\n\\n          gtag('config', 'G-XR7V7MF96T');\\n        \"}],[\"$\",\"div\",null,{\"className\":\"w-full max-w-3xl flex-grow\",\"children\":[[\"$\",\"div\",null,{\"className\":\"navbar\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex-1\",\"children\":[\"$\",\"$L7\",null,{\"className\":\"p-2 hover:underline md:text-2xl font-bold flex gap-4 items-center\",\"href\":\"/\",\"children\":\"Yozzing Blog\"}]}],[\"$\",\"div\",null,{\"className\":\"flex-none\",\"children\":[\"$\",\"div\",null,{\"className\":\"tabs\",\"children\":[[\"$\",\"$L7\",null,{\"className\":\"tab\",\"href\":\"/engineering\",\"children\":\"Engineering\"}],[\"$\",\"$L7\",null,{\"className\":\"tab\",\"href\":\"/essay\",\"children\":\"Essay\"}],[\"$\",\"a\",null,{\"className\":\"tab\",\"href\":\"https://github.com/ddalpange\",\"children\":[\"$\",\"div\",null,{\"className\":\"avatar\",\"children\":[\"$\",\"div\",null,{\"className\":\"w-8 rounded-full\",\"children\":[\"$\",\"img\",null,{\"alt\":\"yozzing\",\"src\":\"/blog/images/profile.jpeg\"}]}]}]}]]}]}]]}],[\"$\",\"div\",null,{\"className\":\"p-4 pb-10\",\"children\":[\"$undefined\",[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]]]}]]}]]}]]}]],\"asNotFound\":false,\"children\":[[\"$\",\"html\",null,{\"lang\":\"en\",\"data-theme\":\"retro\",\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://cdn.jsdelivr.net\"}],[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"type\":\"text/css\",\"href\":\"https://cdn.jsdelivr.net/gh/orioncactus/pretendard/dist/web/variable/pretendardvariable.css\"}]]}],[\"$\",\"body\",null,{\"className\":\"flex flex-col items-center h-[100svh]\",\"children\":[[\"$\",\"$L6\",null,{\"src\":\"https://www.googletagmanager.com/gtag/js?id=G-XR7V7MF96T\",\"strategy\":\"afterInteractive\"}],[\"$\",\"$L6\",null,{\"id\":\"google-analytics\",\"strategy\":\"afterInteractive\",\"children\":\"\\n          window.dataLayer = window.dataLayer || [];\\n          function gtag(){window.dataLayer.push(arguments);}\\n          gtag('js', new Date());\\n\\n          gtag('config', 'G-XR7V7MF96T');\\n        \"}],[\"$\",\"div\",null,{\"className\":\"w-full max-w-3xl flex-grow\",\"children\":[[\"$\",\"div\",null,{\"className\":\"navbar\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex-1\",\"children\":[\"$\",\"$L7\",null,{\"className\":\"p-2 hover:underline md:text-2xl font-bold flex gap-4 items-center\",\"href\":\"/\",\"children\":\"Yozzing Blog\"}]}],[\"$\",\"div\",null,{\"className\":\"flex-none\",\"children\":[\"$\",\"div\",null,{\"className\":\"tabs\",\"children\":[[\"$\",\"$L7\",null,{\"className\":\"tab\",\"href\":\"/engineering\",\"children\":\"Engineering\"}],[\"$\",\"$L7\",null,{\"className\":\"tab\",\"href\":\"/essay\",\"children\":\"Essay\"}],[\"$\",\"a\",null,{\"className\":\"tab\",\"href\":\"https://github.com/ddalpange\",\"children\":[\"$\",\"div\",null,{\"className\":\"avatar\",\"children\":[\"$\",\"div\",null,{\"className\":\"w-8 rounded-full\",\"children\":[\"$\",\"img\",null,{\"alt\":\"yozzing\",\"src\":\"/blog/images/profile.jpeg\"}]}]}]}]]}]}]]}],[\"$\",\"div\",null,{\"className\":\"p-4 pb-10\",\"children\":[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":\"$undefined\",\"asNotFound\":false,\"childProp\":{\"current\":[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"asNotFound\":false,\"childProp\":{\"current\":[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\",[\"slug\",\"2017-12-22-linux-wget\",\"d\"],\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"hasLoading\":false,\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"asNotFound\":false,\"childProp\":{\"current\":[\"$La\",\"$@b\"],\"segment\":\"__PAGE__?{\\\"slug\\\":\\\"2017-12-22-linux-wget\\\"}\"}}],\"segment\":[\"slug\",\"2017-12-22-linux-wget\",\"d\"]}}],\"segment\":\"blog\"}}]}]]}]]}]]}],\"$@c\"]}]\n"])</script><script>self.__next_f.push([1,"d:I{\"id\":\"3969\",\"name\":\"Disqus\",\"chunks\":[\"308:app/blog/[slug]/page-a59e512fe2b74578\"],\"async\":false}\n5:[null,null,[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/4640374ce090311f.css\",\"precedence\":\"next.js\"}]]]\n"])</script><script>self.__next_f.push([1,"a:[\"$\",\"article\",null,{\"className\":\"prose max-w-full flex flex-col items-center\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mb-2\",\"children\":\"over 5 years ago\"}],[\"$\",\"h1\",null,{\"children\":\"리눅스 wget 명령어\"}],[\"$\",\"div\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"prose self-center\",\"children\":[[\"$\",\"h3\",\"0\",{\"children\":\"설명\"}],\"\\n\",[\"$\",\"p\",\"2\",{\"children\":\"CUI환경에서 파일을 다운받을때 사용한다.\"}],\"\\n\",[\"$\",\"h3\",\"4\",{\"children\":\"도움말\"}],\"\\n\",[\"$\",\"pre\",\"6\",{\"children\":[\"$\",\"code\",null,{\"children\":\"$$ wget -help\\n\\nGNU Wget 1.17.1, a non-interactive network retriever.\\nUsage: wget [OPTION]... [URL]...\\n\\nMandatory arguments to long options are mandatory for short options too.\\n\\nStartup:\\n  -V,  --version                   display the version of Wget and exit\\n  -h,  --help                      print this help\\n  -b,  --background                go to background after startup\\n  -e,  --execute=COMMAND           execute a `.wgetrc'-style command\\n\\nLogging and input file:\\n  -o,  --output-file=FILE          log messages to FILE\\n  -a,  --append-output=FILE        append messages to FILE\\n  -d,  --debug                     print lots of debugging information\\n  -q,  --quiet                     quiet (no output)\\n  -v,  --verbose                   be verbose (this is the default)\\n  -nv, --no-verbose                turn off verboseness, without being quiet\\n       --report-speed=TYPE         output bandwidth as TYPE.  TYPE can be bits\\n  -i,  --input-file=FILE           download URLs found in local or external FILE\\n  -F,  --force-html                treat input file as HTML\\n  -B,  --base=URL                  resolves HTML input-file links (-i -F)\\n                                     relative to URL\\n       --config=FILE               specify config file to use\\n       --no-config                 do not read any config file\\n       --rejected-log=FILE         log reasons for URL rejection to FILE\\n\\nDownload:\\n  -t,  --tries=NUMBER              set number of retries to NUMBER (0 unlimits)\\n       --retry-connrefused         retry even if connection is refused\\n  -O,  --output-document=FILE      write documents to FILE\\n  -nc, --no-clobber                skip downloads that would download to\\n                                     existing files (overwriting them)\\n  -c,  --continue                  resume getting a partially-downloaded file\\n       --start-pos=OFFSET          start downloading from zero-based position OFFSET\\n       --progress=TYPE             select progress gauge type\\n       --show-progress             display the progress bar in any verbosity mode\\n  -N,  --timestamping              don't re-retrieve files unless newer than\\n                                     local\\n       --no-if-modified-since      don't use conditional if-modified-since get\\n                                     requests in timestamping mode\\n       --no-use-server-timestamps  don't set the local file's timestamp by\\n                                     the one on the server\\n  -S,  --server-response           print server response\\n       --spider                    don't download anything\\n  -T,  --timeout=SECONDS           set all timeout values to SECONDS\\n       --dns-timeout=SECS          set the DNS lookup timeout to SECS\\n       --connect-timeout=SECS      set the connect timeout to SECS\\n       --read-timeout=SECS         set the read timeout to SECS\\n  -w,  --wait=SECONDS              wait SECONDS between retrievals\\n       --waitretry=SECONDS         wait 1..SECONDS between retries of a retrieval\\n       --random-wait               wait from 0.5*WAIT...1.5*WAIT secs between retrievals\\n       --no-proxy                  explicitly turn off proxy\\n  -Q,  --quota=NUMBER              set retrieval quota to NUMBER\\n       --bind-address=ADDRESS      bind to ADDRESS (hostname or IP) on local host\\n       --limit-rate=RATE           limit download rate to RATE\\n       --no-dns-cache              disable caching DNS lookups\\n       --restrict-file-names=OS    restrict chars in file names to ones OS allows\\n       --ignore-case               ignore case when matching files/directories\\n  -4,  --inet4-only                connect only to IPv4 addresses\\n  -6,  --inet6-only                connect only to IPv6 addresses\\n       --prefer-family=FAMILY      connect first to addresses of specified family,\\n                                     one of IPv6, IPv4, or none\\n       --user=USER                 set both ftp and http user to USER\\n       --password=PASS             set both ftp and http password to PASS\\n       --ask-password              prompt for passwords\\n       --no-iri                    turn off IRI support\\n       --local-encoding=ENC        use ENC as the local encoding for IRIs\\n       --remote-encoding=ENC       use ENC as the default remote encoding\\n       --unlink                    remove file before clobber\\n\\nDirectories:\\n  -nd, --no-directories            don't create directories\\n  -x,  --force-directories         force creation of directories\\n  -nH, --no-host-directories       don't create host directories\\n       --protocol-directories      use protocol name in directories\\n  -P,  --directory-prefix=PREFIX   save files to PREFIX/..\\n       --cut-dirs=NUMBER           ignore NUMBER remote directory components\\n\\nHTTP options:\\n       --http-user=USER            set http user to USER\\n       --http-password=PASS        set http password to PASS\\n       --no-cache                  disallow server-cached data\\n       --default-page=NAME         change the default page name (normally\\n                                     this is 'index.html'.)\\n  -E,  --adjust-extension          save HTML/CSS documents with proper extensions\\n       --ignore-length             ignore 'Content-Length' header field\\n       --header=STRING             insert STRING among the headers\\n       --max-redirect              maximum redirections allowed per page\\n       --proxy-user=USER           set USER as proxy username\\n       --proxy-password=PASS       set PASS as proxy password\\n       --referer=URL               include 'Referer: URL' header in HTTP request\\n       --save-headers              save the HTTP headers to file\\n  -U,  --user-agent=AGENT          identify as AGENT instead of Wget/VERSION\\n       --no-http-keep-alive        disable HTTP keep-alive (persistent connections)\\n       --no-cookies                don't use cookies\\n       --load-cookies=FILE         load cookies from FILE before session\\n       --save-cookies=FILE         save cookies to FILE after session\\n       --keep-session-cookies      load and save session (non-permanent) cookies\\n       --post-data=STRING          use the POST method; send STRING as the data\\n       --post-file=FILE            use the POST method; send contents of FILE\\n       --method=HTTPMethod         use method \\\"HTTPMethod\\\" in the request\\n       --body-data=STRING          send STRING as data. --method MUST be set\\n       --body-file=FILE            send contents of FILE. --method MUST be set\\n       --content-disposition       honor the Content-Disposition header when\\n                                     choosing local file names (EXPERIMENTAL)\\n       --content-on-error          output the received content on server errors\\n       --auth-no-challenge         send Basic HTTP authentication information\\n                                     without first waiting for the server's\\n                                     challenge\\n\\nHTTPS (SSL/TLS) options:\\n       --secure-protocol=PR        choose secure protocol, one of auto, SSLv2,\\n                                     SSLv3, TLSv1 and PFS\\n       --https-only                only follow secure HTTPS links\\n       --no-check-certificate      don't validate the server's certificate\\n       --certificate=FILE          client certificate file\\n       --certificate-type=TYPE     client certificate type, PEM or DER\\n       --private-key=FILE          private key file\\n       --private-key-type=TYPE     private key type, PEM or DER\\n       --ca-certificate=FILE       file with the bundle of CAs\\n       --ca-directory=DIR          directory where hash list of CAs is stored\\n       --crl-file=FILE             file with bundle of CRLs\\n       --random-file=FILE          file with random data for seeding the SSL PRNG\\n       --egd-file=FILE             file naming the EGD socket with random data\\n\\nHSTS options:\\n       --no-hsts                   disable HSTS\\n       --hsts-file                 path of HSTS database (will override default)\\n\\nFTP options:\\n       --ftp-user=USER             set ftp user to USER\\n       --ftp-password=PASS         set ftp password to PASS\\n       --no-remove-listing         don't remove '.listing' files\\n       --no-glob                   turn off FTP file name globbing\\n       --no-passive-ftp            disable the \\\"passive\\\" transfer mode\\n       --preserve-permissions      preserve remote file permissions\\n       --retr-symlinks             when recursing, get linked-to files (not dir)\\n\\nFTPS options:\\n       --ftps-implicit                 use implicit FTPS (default port is 990)\\n       --ftps-resume-ssl               resume the SSL/TLS session started in the control connection when\\n                                         opening a data connection\\n       --ftps-clear-data-connection    cipher the control channel only; all the data will be in plaintext\\n       --ftps-fallback-to-ftp          fall back to FTP if FTPS is not supported in the target server\\nWARC options:\\n       --warc-file=FILENAME        save request/response data to a .warc.gz file\\n       --warc-header=STRING        insert STRING into the warcinfo record\\n       --warc-max-size=NUMBER      set maximum size of WARC files to NUMBER\\n       --warc-cdx                  write CDX index files\\n       --warc-dedup=FILENAME       do not store records listed in this CDX file\\n       --no-warc-compression       do not compress WARC files with GZIP\\n       --no-warc-digests           do not calculate SHA1 digests\\n       --no-warc-keep-log          do not store the log file in a WARC record\\n       --warc-tempdir=DIRECTORY    location for temporary files created by the\\n                                     WARC writer\\n\\nRecursive download:\\n  -r,  --recursive                 specify recursive download\\n  -l,  --level=NUMBER              maximum recursion depth (inf or 0 for infinite)\\n       --delete-after              delete files locally after downloading them\\n  -k,  --convert-links             make links in downloaded HTML or CSS point to\\n                                     local files\\n       --convert-file-only         convert the file part of the URLs only (usually known as the basename)\\n       --backups=N                 before writing file X, rotate up to N backup files\\n  -K,  --backup-converted          before converting file X, back up as X.orig\\n  -m,  --mirror                    shortcut for -N -r -l inf --no-remove-listing\\n  -p,  --page-requisites           get all images, etc. needed to display HTML page\\n       --strict-comments           turn on strict (SGML) handling of HTML comments\\n\\nRecursive accept/reject:\\n  -A,  --accept=LIST               comma-separated list of accepted extensions\\n  -R,  --reject=LIST               comma-separated list of rejected extensions\\n       --accept-regex=REGEX        regex matching accepted URLs\\n       --reject-regex=REGEX        regex matching rejected URLs\\n       --regex-type=TYPE           regex type (posix|pcre)\\n  -D,  --domains=LIST              comma-separated list of accepted domains\\n       --exclude-domains=LIST      comma-separated list of rejected domains\\n       --follow-ftp                follow FTP links from HTML documents\\n       --follow-tags=LIST          comma-separated list of followed HTML tags\\n       --ignore-tags=LIST          comma-separated list of ignored HTML tags\\n  -H,  --span-hosts                go to foreign hosts when recursive\\n  -L,  --relative                  follow relative links only\\n  -I,  --include-directories=LIST  list of allowed directories\\n       --trust-server-names        use the name specified by the redirection\\n                                     URL's last component\\n  -X,  --exclude-directories=LIST  list of excluded directories\\n  -np, --no-parent                 don't ascend to the parent directory\\n\\nMail bug reports and suggestions to \u003cbug-wget@gnu.org\u003e\\n\"}]}],\"\\n\",[\"$\",\"h3\",\"8\",{\"children\":\"활용 예\"}],\"\\n\",[\"$\",\"p\",\"10\",{\"children\":\"영상 다운로드서버에 올라가있는 영상들이 정상적인지를 체크하기위해 한번씩 다운로드해준다.\"}],\"\\n\",[\"$\",\"p\",\"12\",{\"children\":[\"$\",\"em\",null,{\"children\":\"list.txt\"}]}],\"\\n\",[\"$\",\"pre\",\"14\",{\"children\":[\"$\",\"code\",null,{\"children\":\" http://111.111.11.1/assets/prod/1629/ORIGINAL/1629_1504176178478.ts\\n http://111.111.11.1/assets/prod/1630/ORIGINAL/1630_1504176206826.ts\\n http://1111.111.11.1/assets/prod/1672/ORIGINAL/1672_1505466475322.ts\\n 엄청많아 100개가 넘어 ..\\n\"}]}],\"\\n\",[\"$\",\"pre\",\"16\",{\"children\":[\"$\",\"code\",null,{\"children\":\"$$ wget -i list.txt -o log --no-verbose\\n\"}]}],\"\\n\",[\"$\",\"p\",\"18\",{\"children\":\"list에 써져있는 파일들을 차례대로 다운로드한 후 로그를 남긴다(성공여부만 필요)\"}],\"\\n\",[\"$\",\"p\",\"20\",{\"children\":\"-i 인풋\\n-o 아웃풋\\n--no-verbose 다 남길 필요없다.\"}],\"\\n\",[\"$\",\"p\",\"22\",{\"children\":[\"$\",\"em\",null,{\"children\":\"log\"}]}],\"\\n\",[\"$\",\"pre\",\"24\",{\"children\":[\"$\",\"code\",null,{\"children\":\"2017-12-22 17:17:53 URL:http://111.111.11.1/assets/prod/1629/ORIGINAL/1629_1504176178478.ts [16958540/16958540] -\u003e \\\"1629_1504176178478.ts.2\\\" [1]\\n2017-12-22 17:17:56 URL:http://111.111.11.1/assets/prod/1630/ORIGINAL/1630_1504176206826.ts [32955836/32955836] -\u003e \\\"1630_1504176206826.ts.2\\\" [1]\\n2017-12-22 17:17:59 URL:http://111.111.11.1/assets/prod/1672/ORIGINAL/1672_1505466475322.ts [32954520/32954520] -\u003e \\\"1672_1505466475322.ts.2\\\" [1]\\n2017-12-22 17:18:03 URL:http://111.111.11.1/assets/uplus-assets/prod/1686/ORIGINAL/1686_1505969267788.ts [32950572/32950572] -\u003e \\\"1686_1505969267788.ts.2\\\" [1]\\nhttp://111.111.11.1/assets/prod/1692/ORIGINAL/1692_1506023232326203376.ts:\\n2017-12-22 17:18:03 ERROR 404: Not Found.\\nFINISHED --2017-12-22 17:18:03--\\nTotal wall clock time: 12s\\nDownloaded: 4 files, 110M in 11s (9.61 MB/s)\\n\"}]}],\"\\n\",[\"$\",\"p\",\"26\",{\"children\":\"없으면 저렇게 404 Not Found 뜬다.\"}],\"\\n\",[\"$\",\"p\",\"28\",{\"children\":\"20개정도 브라우저에 입력하다가 빡쳐서 찾았다.\"}],\"\\n\"]}]}],[\"$\",\"$Ld\",null,{\"post\":{\"title\":\"리눅스 wget 명령어\",\"date\":\"2017-12-22T23:53:22.000Z\",\"thumbnail\":\"https://upload.wikimedia.org/wikipedia/commons/6/60/Wget_1.13.4.png\",\"banner\":\"https://upload.wikimedia.org/wikipedia/commons/6/60/Wget_1.13.4.png\",\"toc\":true,\"categories\":[\"Linux\"],\"tags\":[\"Linux\",\"Wget\"],\"content\":\"\\n### 설명\\n\\nCUI환경에서 파일을 다운받을때 사용한다.\\n\\n### 도움말\\n\\n\u003c!-- more --\u003e\\n\\n```shell\\n$ wget -help\\n\\nGNU Wget 1.17.1, a non-interactive network retriever.\\nUsage: wget [OPTION]... [URL]...\\n\\nMandatory arguments to long options are mandatory for short options too.\\n\\nStartup:\\n  -V,  --version                   display the version of Wget and exit\\n  -h,  --help                      print this help\\n  -b,  --background                go to background after startup\\n  -e,  --execute=COMMAND           execute a `.wgetrc'-style command\\n\\nLogging and input file:\\n  -o,  --output-file=FILE          log messages to FILE\\n  -a,  --append-output=FILE        append messages to FILE\\n  -d,  --debug                     print lots of debugging information\\n  -q,  --quiet                     quiet (no output)\\n  -v,  --verbose                   be verbose (this is the default)\\n  -nv, --no-verbose                turn off verboseness, without being quiet\\n       --report-speed=TYPE         output bandwidth as TYPE.  TYPE can be bits\\n  -i,  --input-file=FILE           download URLs found in local or external FILE\\n  -F,  --force-html                treat input file as HTML\\n  -B,  --base=URL                  resolves HTML input-file links (-i -F)\\n                                     relative to URL\\n       --config=FILE               specify config file to use\\n       --no-config                 do not read any config file\\n       --rejected-log=FILE         log reasons for URL rejection to FILE\\n\\nDownload:\\n  -t,  --tries=NUMBER              set number of retries to NUMBER (0 unlimits)\\n       --retry-connrefused         retry even if connection is refused\\n  -O,  --output-document=FILE      write documents to FILE\\n  -nc, --no-clobber                skip downloads that would download to\\n                                     existing files (overwriting them)\\n  -c,  --continue                  resume getting a partially-downloaded file\\n       --start-pos=OFFSET          start downloading from zero-based position OFFSET\\n       --progress=TYPE             select progress gauge type\\n       --show-progress             display the progress bar in any verbosity mode\\n  -N,  --timestamping              don't re-retrieve files unless newer than\\n                                     local\\n       --no-if-modified-since      don't use conditional if-modified-since get\\n                                     requests in timestamping mode\\n       --no-use-server-timestamps  don't set the local file's timestamp by\\n                                     the one on the server\\n  -S,  --server-response           print server response\\n       --spider                    don't download anything\\n  -T,  --timeout=SECONDS           set all timeout values to SECONDS\\n       --dns-timeout=SECS          set the DNS lookup timeout to SECS\\n       --connect-timeout=SECS      set the connect timeout to SECS\\n       --read-timeout=SECS         set the read timeout to SECS\\n  -w,  --wait=SECONDS              wait SECONDS between retrievals\\n       --waitretry=SECONDS         wait 1..SECONDS between retries of a retrieval\\n       --random-wait               wait from 0.5*WAIT...1.5*WAIT secs between retrievals\\n       --no-proxy                  explicitly turn off proxy\\n  -Q,  --quota=NUMBER              set retrieval quota to NUMBER\\n       --bind-address=ADDRESS      bind to ADDRESS (hostname or IP) on local host\\n       --limit-rate=RATE           limit download rate to RATE\\n       --no-dns-cache              disable caching DNS lookups\\n       --restrict-file-names=OS    restrict chars in file names to ones OS allows\\n       --ignore-case               ignore case when matching files/directories\\n  -4,  --inet4-only                connect only to IPv4 addresses\\n  -6,  --inet6-only                connect only to IPv6 addresses\\n       --prefer-family=FAMILY      connect first to addresses of specified family,\\n                                     one of IPv6, IPv4, or none\\n       --user=USER                 set both ftp and http user to USER\\n       --password=PASS             set both ftp and http password to PASS\\n       --ask-password              prompt for passwords\\n       --no-iri                    turn off IRI support\\n       --local-encoding=ENC        use ENC as the local encoding for IRIs\\n       --remote-encoding=ENC       use ENC as the default remote encoding\\n       --unlink                    remove file before clobber\\n\\nDirectories:\\n  -nd, --no-directories            don't create directories\\n  -x,  --force-directories         force creation of directories\\n  -nH, --no-host-directories       don't create host directories\\n       --protocol-directories      use protocol name in directories\\n  -P,  --directory-prefix=PREFIX   save files to PREFIX/..\\n       --cut-dirs=NUMBER           ignore NUMBER remote directory components\\n\\nHTTP options:\\n       --http-user=USER            set http user to USER\\n       --http-password=PASS        set http password to PASS\\n       --no-cache                  disallow server-cached data\\n       --default-page=NAME         change the default page name (normally\\n                                     this is 'index.html'.)\\n  -E,  --adjust-extension          save HTML/CSS documents with proper extensions\\n       --ignore-length             ignore 'Content-Length' header field\\n       --header=STRING             insert STRING among the headers\\n       --max-redirect              maximum redirections allowed per page\\n       --proxy-user=USER           set USER as proxy username\\n       --proxy-password=PASS       set PASS as proxy password\\n       --referer=URL               include 'Referer: URL' header in HTTP request\\n       --save-headers              save the HTTP headers to file\\n  -U,  --user-agent=AGENT          identify as AGENT instead of Wget/VERSION\\n       --no-http-keep-alive        disable HTTP keep-alive (persistent connections)\\n       --no-cookies                don't use cookies\\n       --load-cookies=FILE         load cookies from FILE before session\\n       --save-cookies=FILE         save cookies to FILE after session\\n       --keep-session-cookies      load and save session (non-permanent) cookies\\n       --post-data=STRING          use the POST method; send STRING as the data\\n       --post-file=FILE            use the POST method; send contents of FILE\\n       --method=HTTPMethod         use method \\\"HTTPMethod\\\" in the request\\n       --body-data=STRING          send STRING as data. --method MUST be set\\n       --body-file=FILE            send contents of FILE. --method MUST be set\\n       --content-disposition       honor the Content-Disposition header when\\n                                     choosing local file names (EXPERIMENTAL)\\n       --content-on-error          output the received content on server errors\\n       --auth-no-challenge         send Basic HTTP authentication information\\n                                     without first waiting for the server's\\n                                     challenge\\n\\nHTTPS (SSL/TLS) options:\\n       --secure-protocol=PR        choose secure protocol, one of auto, SSLv2,\\n                                     SSLv3, TLSv1 and PFS\\n       --https-only                only follow secure HTTPS links\\n       --no-check-certificate      don't validate the server's certificate\\n       --certificate=FILE          client certificate file\\n       --certificate-type=TYPE     client certificate type, PEM or DER\\n       --private-key=FILE          private key file\\n       --private-key-type=TYPE     private key type, PEM or DER\\n       --ca-certificate=FILE       file with the bundle of CAs\\n       --ca-directory=DIR          directory where hash list of CAs is stored\\n       --crl-file=FILE             file with bundle of CRLs\\n       --random-file=FILE          file with random data for seeding the SSL PRNG\\n       --egd-file=FILE             file naming the EGD socket with random data\\n\\nHSTS options:\\n       --no-hsts                   disable HSTS\\n       --hsts-file                 path of HSTS database (will override default)\\n\\nFTP options:\\n       --ftp-user=USER             set ftp user to USER\\n       --ftp-password=PASS         set ftp password to PASS\\n       --no-remove-listing         don't remove '.listing' files\\n       --no-glob                   turn off FTP file name globbing\\n       --no-passive-ftp            disable the \\\"passive\\\" transfer mode\\n       --preserve-permissions      preserve remote file permissions\\n       --retr-symlinks             when recursing, get linked-to files (not dir)\\n\\nFTPS options:\\n       --ftps-implicit                 use implicit FTPS (default port is 990)\\n       --ftps-resume-ssl               resume the SSL/TLS session started in the control connection when\\n                                         opening a data connection\\n       --ftps-clear-data-connection    cipher the control channel only; all the data will be in plaintext\\n       --ftps-fallback-to-ftp          fall back to FTP if FTPS is not supported in the target server\\nWARC options:\\n       --warc-file=FILENAME        save request/response data to a .warc.gz file\\n       --warc-header=STRING        insert STRING into the warcinfo record\\n       --warc-max-size=NUMBER      set maximum size of WARC files to NUMBER\\n       --warc-cdx                  write CDX index files\\n       --warc-dedup=FILENAME       do not store records listed in this CDX file\\n       --no-warc-compression       do not compress WARC files with GZIP\\n       --no-warc-digests           do not calculate SHA1 digests\\n       --no-warc-keep-log          do not store the log file in a WARC record\\n       --warc-tempdir=DIRECTORY    location for temporary files created by the\\n                                     WARC writer\\n\\nRecursive download:\\n  -r,  --recursive                 specify recursive download\\n  -l,  --level=NUMBER              maximum recursion depth (inf or 0 for infinite)\\n       --delete-after              delete files locally after downloading them\\n  -k,  --convert-links             make links in downloaded HTML or CSS point to\\n                                     local files\\n       --convert-file-only         convert the file part of the URLs only (usually known as the basename)\\n       --backups=N                 before writing file X, rotate up to N backup files\\n  -K,  --backup-converted          before converting file X, back up as X.orig\\n  -m,  --mirror                    shortcut for -N -r -l inf --no-remove-listing\\n  -p,  --page-requisites           get all images, etc. needed to display HTML page\\n       --strict-comments           turn on strict (SGML) handling of HTML comments\\n\\nRecursive accept/reject:\\n  -A,  --accept=LIST               comma-separated list of accepted extensions\\n  -R,  --reject=LIST               comma-separated list of rejected extensions\\n       --accept-regex=REGEX        regex matching accepted URLs\\n       --reject-regex=REGEX        regex matching rejected URLs\\n       --regex-type=TYPE           regex type (posix|pcre)\\n  -D,  --domains=LIST              comma-separated list of accepted domains\\n       --exclude-domains=LIST      comma-separated list of rejected domains\\n       --follow-ftp                follow FTP links from HTML documents\\n       --follow-tags=LIST          comma-separated list of followed HTML tags\\n       --ignore-tags=LIST          comma-separated list of ignored HTML tags\\n  -H,  --span-hosts                go to foreign hosts when recursive\\n  -L,  --relative                  follow relative links only\\n  -I,  --include-directories=LIST  list of allowed directories\\n       --trust-server-names        use the name specified by the redirection\\n                                     URL's last component\\n  -X,  --exclude-directories=LIST  list of excluded directories\\n  -np, --no-parent                 don't ascend to the parent directory\\n\\nMail bug reports and suggestions to \u003cbug-wget@gnu.org\u003e\\n```\\n\\n### 활용 예\\n\\n영상 다운로드서버에 올라가있는 영상들이 정상적인지를 체크하기위해 한번씩 다운로드해준다.\\n\\n*list.txt*\\n\\n```\\n http://111.111.11.1/assets/prod/1629/ORIGINAL/1629_1504176178478.ts\\n http://111.111.11.1/assets/prod/1630/ORIGINAL/1630_1504176206826.ts\\n http://1111.111.11.1/assets/prod/1672/ORIGINAL/1672_1505466475322.ts\\n 엄청많아 100개가 넘어 ..\\n```\\n\\n```sh\\n$ wget -i list.txt -o log --no-verbose\\n```\\n\\nlist에 써져있는 파일들을 차례대로 다운로드한 후 로그를 남긴다(성공여부만 필요)\\n\\n-i 인풋\\n-o 아웃풋\\n--no-verbose 다 남길 필요없다.\\n\\n*log*\\n```\\n2017-12-22 17:17:53 URL:http://111.111.11.1/assets/prod/1629/ORIGINAL/1629_1504176178478.ts [16958540/16958540] -\u003e \\\"1629_1504176178478.ts.2\\\" [1]\\n2017-12-22 17:17:56 URL:http://111.111.11.1/assets/prod/1630/ORIGINAL/1630_1504176206826.ts [32955836/32955836] -\u003e \\\"1630_1504176206826.ts.2\\\" [1]\\n2017-12-22 17:17:59 URL:http://111.111.11.1/assets/prod/1672/ORIGINAL/1672_1505466475322.ts [32954520/32954520] -\u003e \\\"1672_1505466475322.ts.2\\\" [1]\\n2017-12-22 17:18:03 URL:http://111.111.11.1/assets/uplus-assets/prod/1686/ORIGINAL/1686_1505969267788.ts [32950572/32950572] -\u003e \\\"1686_1505969267788.ts.2\\\" [1]\\nhttp://111.111.11.1/assets/prod/1692/ORIGINAL/1692_1506023232326203376.ts:\\n2017-12-22 17:18:03 ERROR 404: Not Found.\\nFINISHED --2017-12-22 17:18:03--\\nTotal wall clock time: 12s\\nDownloaded: 4 files, 110M in 11s (9.61 MB/s)\\n```\\n\\n없으면 저렇게 404 Not Found 뜬다.\\n\\n20개정도 브라우저에 입력하다가 빡쳐서 찾았다.\\n\",\"contentHtml\":\"\u003ch3\u003e설명\u003c/h3\u003e\\n\u003cp\u003eCUI환경에서 파일을 다운받을때 사용한다.\u003c/p\u003e\\n\u003ch3\u003e도움말\u003c/h3\u003e\\n\u003cpre\u003e\u003ccode\u003e$ wget -help\\n\\nGNU Wget 1.17.1, a non-interactive network retriever.\\nUsage: wget [OPTION]... [URL]...\\n\\nMandatory arguments to long options are mandatory for short options too.\\n\\nStartup:\\n  -V,  --version                   display the version of Wget and exit\\n  -h,  --help                      print this help\\n  -b,  --background                go to background after startup\\n  -e,  --execute=COMMAND           execute a `.wgetrc'-style command\\n\\nLogging and input file:\\n  -o,  --output-file=FILE          log messages to FILE\\n  -a,  --append-output=FILE        append messages to FILE\\n  -d,  --debug                     print lots of debugging information\\n  -q,  --quiet                     quiet (no output)\\n  -v,  --verbose                   be verbose (this is the default)\\n  -nv, --no-verbose                turn off verboseness, without being quiet\\n       --report-speed=TYPE         output bandwidth as TYPE.  TYPE can be bits\\n  -i,  --input-file=FILE           download URLs found in local or external FILE\\n  -F,  --force-html                treat input file as HTML\\n  -B,  --base=URL                  resolves HTML input-file links (-i -F)\\n                                     relative to URL\\n       --config=FILE               specify config file to use\\n       --no-config                 do not read any config file\\n       --rejected-log=FILE         log reasons for URL rejection to FILE\\n\\nDownload:\\n  -t,  --tries=NUMBER              set number of retries to NUMBER (0 unlimits)\\n       --retry-connrefused         retry even if connection is refused\\n  -O,  --output-document=FILE      write documents to FILE\\n  -nc, --no-clobber                skip downloads that would download to\\n                                     existing files (overwriting them)\\n  -c,  --continue                  resume getting a partially-downloaded file\\n       --start-pos=OFFSET          start downloading from zero-based position OFFSET\\n       --progress=TYPE             select progress gauge type\\n       --show-progress             display the progress bar in any verbosity mode\\n  -N,  --timestamping              don't re-retrieve files unless newer than\\n                                     local\\n       --no-if-modified-since      don't use conditional if-modified-since get\\n                                     requests in timestamping mode\\n       --no-use-server-timestamps  don't set the local file's timestamp by\\n                                     the one on the server\\n  -S,  --server-response           print server response\\n       --spider                    don't download anything\\n  -T,  --timeout=SECONDS           set all timeout values to SECONDS\\n       --dns-timeout=SECS          set the DNS lookup timeout to SECS\\n       --connect-timeout=SECS      set the connect timeout to SECS\\n       --read-timeout=SECS         set the read timeout to SECS\\n  -w,  --wait=SECONDS              wait SECONDS between retrievals\\n       --waitretry=SECONDS         wait 1..SECONDS between retries of a retrieval\\n       --random-wait               wait from 0.5*WAIT...1.5*WAIT secs between retrievals\\n       --no-proxy                  explicitly turn off proxy\\n  -Q,  --quota=NUMBER              set retrieval quota to NUMBER\\n       --bind-address=ADDRESS      bind to ADDRESS (hostname or IP) on local host\\n       --limit-rate=RATE           limit download rate to RATE\\n       --no-dns-cache              disable caching DNS lookups\\n       --restrict-file-names=OS    restrict chars in file names to ones OS allows\\n       --ignore-case               ignore case when matching files/directories\\n  -4,  --inet4-only                connect only to IPv4 addresses\\n  -6,  --inet6-only                connect only to IPv6 addresses\\n       --prefer-family=FAMILY      connect first to addresses of specified family,\\n                                     one of IPv6, IPv4, or none\\n       --user=USER                 set both ftp and http user to USER\\n       --password=PASS             set both ftp and http password to PASS\\n       --ask-password              prompt for passwords\\n       --no-iri                    turn off IRI support\\n       --local-encoding=ENC        use ENC as the local encoding for IRIs\\n       --remote-encoding=ENC       use ENC as the default remote encoding\\n       --unlink                    remove file before clobber\\n\\nDirectories:\\n  -nd, --no-directories            don't create directories\\n  -x,  --force-directories         force creation of directories\\n  -nH, --no-host-directories       don't create host directories\\n       --protocol-directories      use protocol name in directories\\n  -P,  --directory-prefix=PREFIX   save files to PREFIX/..\\n       --cut-dirs=NUMBER           ignore NUMBER remote directory components\\n\\nHTTP options:\\n       --http-user=USER            set http user to USER\\n       --http-password=PASS        set http password to PASS\\n       --no-cache                  disallow server-cached data\\n       --default-page=NAME         change the default page name (normally\\n                                     this is 'index.html'.)\\n  -E,  --adjust-extension          save HTML/CSS documents with proper extensions\\n       --ignore-length             ignore 'Content-Length' header field\\n       --header=STRING             insert STRING among the headers\\n       --max-redirect              maximum redirections allowed per page\\n       --proxy-user=USER           set USER as proxy username\\n       --proxy-password=PASS       set PASS as proxy password\\n       --referer=URL               include 'Referer: URL' header in HTTP request\\n       --save-headers              save the HTTP headers to file\\n  -U,  --user-agent=AGENT          identify as AGENT instead of Wget/VERSION\\n       --no-http-keep-alive        disable HTTP keep-alive (persistent connections)\\n       --no-cookies                don't use cookies\\n       --load-cookies=FILE         load cookies from FILE before session\\n       --save-cookies=FILE         save cookies to FILE after session\\n       --keep-session-cookies      load and save session (non-permanent) cookies\\n       --post-data=STRING          use the POST method; send STRING as the data\\n       --post-file=FILE            use the POST method; send contents of FILE\\n       --method=HTTPMethod         use method \\\"HTTPMethod\\\" in the request\\n       --body-data=STRING          send STRING as data. --method MUST be set\\n       --body-file=FILE            send contents of FILE. --method MUST be set\\n       --content-disposition       honor the Content-Disposition header when\\n                                     choosing local file names (EXPERIMENTAL)\\n       --content-on-error          output the received content on server errors\\n       --auth-no-challenge         send Basic HTTP authentication information\\n                                     without first waiting for the server's\\n                                     challenge\\n\\nHTTPS (SSL/TLS) options:\\n       --secure-protocol=PR        choose secure protocol, one of auto, SSLv2,\\n                                     SSLv3, TLSv1 and PFS\\n       --https-only                only follow secure HTTPS links\\n       --no-check-certificate      don't validate the server's certificate\\n       --certificate=FILE          client certificate file\\n       --certificate-type=TYPE     client certificate type, PEM or DER\\n       --private-key=FILE          private key file\\n       --private-key-type=TYPE     private key type, PEM or DER\\n       --ca-certificate=FILE       file with the bundle of CAs\\n       --ca-directory=DIR          directory where hash list of CAs is stored\\n       --crl-file=FILE             file with bundle of CRLs\\n       --random-file=FILE          file with random data for seeding the SSL PRNG\\n       --egd-file=FILE             file naming the EGD socket with random data\\n\\nHSTS options:\\n       --no-hsts                   disable HSTS\\n       --hsts-file                 path of HSTS database (will override default)\\n\\nFTP options:\\n       --ftp-user=USER             set ftp user to USER\\n       --ftp-password=PASS         set ftp password to PASS\\n       --no-remove-listing         don't remove '.listing' files\\n       --no-glob                   turn off FTP file name globbing\\n       --no-passive-ftp            disable the \\\"passive\\\" transfer mode\\n       --preserve-permissions      preserve remote file permissions\\n       --retr-symlinks             when recursing, get linked-to files (not dir)\\n\\nFTPS options:\\n       --ftps-implicit                 use implicit FTPS (default port is 990)\\n       --ftps-resume-ssl               resume the SSL/TLS session started in the control connection when\\n                                         opening a data connection\\n       --ftps-clear-data-connection    cipher the control channel only; all the data will be in plaintext\\n       --ftps-fallback-to-ftp          fall back to FTP if FTPS is not supported in the target server\\nWARC options:\\n       --warc-file=FILENAME        save request/response data to a .warc.gz file\\n       --warc-header=STRING        insert STRING into the warcinfo record\\n       --warc-max-size=NUMBER      set maximum size of WARC files to NUMBER\\n       --warc-cdx                  write CDX index files\\n       --warc-dedup=FILENAME       do not store records listed in this CDX file\\n       --no-warc-compression       do not compress WARC files with GZIP\\n       --no-warc-digests           do not calculate SHA1 digests\\n       --no-warc-keep-log          do not store the log file in a WARC record\\n       --warc-tempdir=DIRECTORY    location for temporary files created by the\\n                                     WARC writer\\n\\nRecursive download:\\n  -r,  --recursive                 specify recursive download\\n  -l,  --level=NUMBER              maximum recursion depth (inf or 0 for infinite)\\n       --delete-after              delete files locally after downloading them\\n  -k,  --convert-links             make links in downloaded HTML or CSS point to\\n                                     local files\\n       --convert-file-only         convert the file part of the URLs only (usually known as the basename)\\n       --backups=N                 before writing file X, rotate up to N backup files\\n  -K,  --backup-converted          before converting file X, back up as X.orig\\n  -m,  --mirror                    shortcut for -N -r -l inf --no-remove-listing\\n  -p,  --page-requisites           get all images, etc. needed to display HTML page\\n       --strict-comments           turn on strict (SGML) handling of HTML comments\\n\\nRecursive accept/reject:\\n  -A,  --accept=LIST               comma-separated list of accepted extensions\\n  -R,  --reject=LIST               comma-separated list of rejected extensions\\n       --accept-regex=REGEX        regex matching accepted URLs\\n       --reject-regex=REGEX        regex matching rejected URLs\\n       --regex-type=TYPE           regex type (posix|pcre)\\n  -D,  --domains=LIST              comma-separated list of accepted domains\\n       --exclude-domains=LIST      comma-separated list of rejected domains\\n       --follow-ftp                follow FTP links from HTML documents\\n       --follow-tags=LIST          comma-separated list of followed HTML tags\\n       --ignore-tags=LIST          comma-separated list of ignored HTML tags\\n  -H,  --span-hosts                go to foreign hosts when recursive\\n  -L,  --relative                  follow relative links only\\n  -I,  --include-directories=LIST  list of allowed directories\\n       --trust-server-names        use the name specified by the redirection\\n                                     URL's last component\\n  -X,  --exclude-directories=LIST  list of excluded directories\\n  -np, --no-parent                 don't ascend to the parent directory\\n\\nMail bug reports and suggestions to \u0026#x3C;bug-wget@gnu.org\u003e\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003ch3\u003e활용 예\u003c/h3\u003e\\n\u003cp\u003e영상 다운로드서버에 올라가있는 영상들이 정상적인지를 체크하기위해 한번씩 다운로드해준다.\u003c/p\u003e\\n\u003cp\u003e\u003cem\u003elist.txt\u003c/em\u003e\u003c/p\u003e\\n\u003cpre\u003e\u003ccode\u003e http://111.111.11.1/assets/prod/1629/ORIGINAL/1629_1504176178478.ts\\n http://111.111.11.1/assets/prod/1630/ORIGINAL/1630_1504176206826.ts\\n http://1111.111.11.1/assets/prod/1672/ORIGINAL/1672_1505466475322.ts\\n 엄청많아 100개가 넘어 ..\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cpre\u003e\u003ccode\u003e$ wget -i list.txt -o log --no-verbose\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003elist에 써져있는 파일들을 차례대로 다운로드한 후 로그를 남긴다(성공여부만 필요)\u003c/p\u003e\\n\u003cp\u003e-i 인풋\\n-o 아웃풋\\n--no-verbose 다 남길 필요없다.\u003c/p\u003e\\n\u003cp\u003e\u003cem\u003elog\u003c/em\u003e\u003c/p\u003e\\n\u003cpre\u003e\u003ccode\u003e2017-12-22 17:17:53 URL:http://111.111.11.1/assets/prod/1629/ORIGINAL/1629_1504176178478.ts [16958540/16958540] -\u003e \\\"1629_1504176178478.ts.2\\\" [1]\\n2017-12-22 17:17:56 URL:http://111.111.11.1/assets/prod/1630/ORIGINAL/1630_1504176206826.ts [32955836/32955836] -\u003e \\\"1630_1504176206826.ts.2\\\" [1]\\n2017-12-22 17:17:59 URL:http://111.111.11.1/assets/prod/1672/ORIGINAL/1672_1505466475322.ts [32954520/32954520] -\u003e \\\"1672_1505466475322.ts.2\\\" [1]\\n2017-12-22 17:18:03 URL:http://111.111.11.1/assets/uplus-assets/prod/1686/ORIGINAL/1686_1505969267788.ts [32950572/32950572] -\u003e \\\"1686_1505969267788.ts.2\\\" [1]\\nhttp://111.111.11.1/assets/prod/1692/ORIGINAL/1692_1506023232326203376.ts:\\n2017-12-22 17:18:03 ERROR 404: Not Found.\\nFINISHED --2017-12-22 17:18:03--\\nTotal wall clock time: 12s\\nDownloaded: 4 files, 110M in 11s (9.61 MB/s)\\n\u003c/code\u003e\u003c/pre\u003e\\n\u003cp\u003e없으면 저렇게 404 Not Found 뜬다.\u003c/p\u003e\\n\u003cp\u003e20개정도 브라우저에 입력하다가 빡쳐서 찾았다.\u003c/p\u003e\\n\",\"summary\":\"\u003ch3\u003e설명\u003c/h3\u003e\\n\u003cp\u003eCUI환경에서 파일을 다운받을때 사용한다.\u003c/p\u003e\\n\u003ch3\u003e도움말\u003c/h3\u003e\\n\",\"slug\":\"2017-12-22-linux-wget\",\"link\":\"/blog/2017-12-22-linux-wget\"}}]]}]\n"])</script><script>self.__next_f.push([1,"b:[null,null,[]]\nc:[null,null,[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/4640374ce090311f.css\",\"precedence\":\"next.js\"}]]]\n"])</script><script>self.__next_f.push([1,"3:[[[\"$\",\"meta\",null,{\"charSet\":\"utf-8\"}],[\"$\",\"title\",null,{\"children\":\"리눅스 wget 명령어\"}],[\"$\",\"meta\",null,{\"name\":\"description\",\"content\":\"\u003ch3\u003e설명\u003c/h3\u003e\\n\u003cp\u003eCUI환경에서 파일을 다운받을때 사용한다.\u003c/p\u003e\\n\u003ch3\u003e도움말\u003c/h3\u003e\\n\"}],null,[[[\"$\",\"link\",null,{\"rel\":\"author\",\"href\":\"https://github.io/ddalpange\"}],[\"$\",\"meta\",null,{\"name\":\"author\",\"content\":\"ddalpange\"}]]],null,null,[\"$\",\"meta\",null,{\"name\":\"keywords\",\"content\":\"Linux,Wget\"}],null,null,null,[\"$\",\"meta\",null,{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],null,null,null,null,null,null,null,null,null,null,[]],[null,null,null,null],null,null,[null,null,null,null,null],null,null,null,null,[null,[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"any\"}]],[],null]]\n"])</script>